{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 深度模型中的优化\n",
    "## 8.1 批量算法和小批量算法\n",
    "机器学习中通常只用训练集整个代价函数中的一部分项来估计代价函数的期望值，原因如下：    \n",
    "1. 因为整个代价函数的计算代价非常大，且回想n个样本均值的标准差是$\\frac{\\delta}{\\sqrt{n}}$,可以看到使用更多样本误差减小的回报是低于线性的，如10000个样本计算量是100个样本的100倍，但是标准差却只降低10倍，所以使用小批量算法收敛更快。    \n",
    "2. 训练集冗余，大部分样本对梯度作出相似的贡献，所以抽取小样本，能保证不冗余。     \n",
    "此外，为了保证无偏估计，小批量中的样本要保持独立，且相邻梯度的样本也要保持独立，不独立意味着采样不随机，会造成偏差过大。     \n",
    "**关于无偏估计**：训练集只有在第一遍被遍历时，才是无偏估计，遵循真实泛化误差的梯度，当到第二遍时，由于它重新抽取已经用过的样本，将会是有偏的，当然，额外的遍历能减小训练误差。       \n",
    "\n",
    "## 8.2 神经网络优化中的挑战       \n",
    "\n",
    "### 8.2.1 病态      \n",
    "主要指Hessian矩阵H的病态。     \n",
    "在对代价函数用泰勒展开时：     \n",
    "&emsp; $f(x_0-\\epsilon g) = f(x_0) - f'(x_0)\\epsilon g + \\frac{1}{2} f''(x_0)(\\epsilon g)^2$        \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$= f(x_0) - g^T \\epsilon g + \\frac{1}{2} g^T H g$    \n",
    "这样在参数移动$\\epsilon g$,梯度会下降$ \\frac{1}{2} \\epsilon^2 g^T H g - \\epsilon g^T g$,当$\\frac{1}{2} \\epsilon^2 g^T H g$大于$\\epsilon g^T g$时，梯度的病态会存在问题，通常梯度范数不会随着训练过程显著减小，甚至增大（不同于正常的理解，认为训练越久，梯度越小），但是$g^T Hg$的增长会超过一个数量级，所以尽管梯度很强，代价函数下降仍很慢，这时候必须**收缩学习率。**    \n",
    "\n",
    "### 8.2.2 局部极小值       \n",
    "如果一个足够大的训练集可以唯一确定一组模型参数，那么该模型被称为**可辨认**的。带有潜变量（如神经网络）的模型通常是不可辨认的，因为通过交换潜变量能得到等价的模型（如交换神经网络单元i和j的传入权重向量和传出权重向量**？**），所以神经网络会有很多歌局部最小点，但是这些局部最小点都具有相同的代价函数值，所以不是问题。     \n",
    "当局部极小值比全局最小值大很多时，就会有隐患，即使这样，实际中的网络也不是寻找全局最小值，而是找到一个代价很小的局部最小点。     \n",
    "\n",
    "### 8.2.3 鞍点  \n",
    "多类随机函数表现出以下性质：**低维空间中，局部极小值很普遍，更高维空间中，局部极小值很罕见，而鞍点很常见**，对应于Hessian矩阵很好理解，鞍点处Hessian矩阵同时具有正负特征值，当然随着维度越高概率越大。      \n",
    "鞍点对牛顿法影响较大，因为牛顿法旨在求解梯度为0的点，这样很有可能会带向鞍点，相比之下梯度下降不会受如此大的影响，因为其旨在朝下坡移动。     \n",
    "\n",
    "### 8.2.4 悬崖和梯度爆炸            \n",
    "多层网络通常存在像悬崖一样的斜率较大区域，对此，很小的步长也可能会带来严重的问题，对此可以使用**启发式梯度截断**，其基本思想源自梯度并没有指明最佳步长，而只说明在无限小区域内的最佳方向，然后通过截断干涉来减小步长。      \n",
    "\n",
    "### 8.2.5 局部和全局结构间的弱对应       \n",
    "局部的优化方向并不是在全局上最好的优化方向。      \n",
    "\n",
    "## 8.3 基本算法     \n",
    "### 8.3.1 随机梯度下降      \n",
    "随机梯度下降的关键是学习率，保证SGD收敛的一个充分条件是：    \n",
    "&emsp; $ \\sum \\limits_{k=1}^{\\infty} \\epsilon_k = \\infty$ 且$\\sum \\limits_{k=1}^{\\infty} \\epsilon_k^2 = \\infty$    \n",
    "通常让学习率随着训练轮数降低，初始学习率的选取通常是用若干学习率训练几轮，选择比最好效果对应的学习率稍大的学习率。    \n",
    "### 8.3.2 动量       \n",
    "动量更新公式：     \n",
    "    &emsp;$ v = \\alpha v - \\epsilon g$     \n",
    "    &emsp;$ \\theta = \\theta + v$     \n",
    "形象化上理解的含义为累积固定方向上的梯度，达到在正确方向上的加速下降，如图所示：    \n",
    "    <img src=\"img/moment_gd.png\" width=300 height=300>     \n",
    "此外，假设参数更新中总是观察到梯度g, 那么会在-g上不停的加速，其步长会接近(等比公式的极限）：    \n",
    "    &emsp; $ \\frac{\\epsilon \\lVert g \\rVert}{1-\\alpha}$       \n",
    "可以理解为动量的超参数为$\\frac{1}{1-\\alpha}$,所以$\\alpha = 0.9$对应着最大速度10倍于梯度下降。     \n",
    "## 8.4 参数初始化\n",
    "参数初始化的重要原则：破坏不同单元之间的对称性。    \n",
    "一些典型的初始化：\n",
    "   * 标准初始化：$W_{i,j} \\sim U(-\\sqrt{\\frac{6}{m+n}},\\sqrt{\\frac{6}{m+n}})$\n",
    "   * 考虑到类似上面的数值初始化，会是所有初始权重具有相同的标准差，在参数很多时会使每个权重很小，为此，提出一种**稀疏初始化**，使每一层恰好初始化k个非零权重，不至于使权重元素随着m减小     \n",
    "  \n",
    "## 8.5 自适应学习率算法。  \n",
    "### 8.5.1 AdaGrad\n",
    "核心思想：在参数空间中更为平缓的倾斜方向会取得更大的进步。   \n",
    " <img src=\"img/Adagrad.png\" width=480 height=360>      \n",
    "虽然AdaGrad能加快找到全局优化的方向，但是累积梯度平方也会导致有效学习率过早和过量的减小。\n",
    "### 8.5.2 RMSProp\n",
    "核心思想： Adagrad一般用于凸问题的快速收敛，当在非凸问题时，学习轨迹可能传过来很多不同的结构导致学习周期变长，而Adagrad过早的收缩学习率会影响后期的学习，针对于此，RMSProp采用指数衰减平均，丢弃遥远的历史。    \n",
    " <img src=\"img/RMSProp.png\" width=480 height=360>.   \n",
    "### 8.5.3 Adam\n",
    "核心思想：Adam可以视为在RMSProp的基础上引入动量，而动量和平方梯度上都做了偏差修正。    \n",
    " <img src=\"img/Adam.png\" width=480 height=360>    \n",
    " \n",
    "**以上所有的优化算法（包括随机梯度下降和引入动量的随机梯度下降）没有绝对的优劣之分，多数情况下取决于对哪个更熟悉，更方便调参**。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 二阶近似方法    \n",
    "### 8.6.1 牛顿法。  \n",
    "基于二阶泰勒展开在某点$\\theta_0$附近来近似$J(\\theta)$的优化方法，忽略了高级导数：    \n",
    "&emsp; $ J(\\theta) \\thickapprox J(\\theta_0) + (\\theta - \\theta_0)^T g+\\frac{1}{2}(\\theta-\\theta_0)^TH(\\theta-\\theta_0)$     \n",
    "求得最大的下降步长后更新为：&emsp;$\\theta^* = \\theta_0 - H^{-1}g$     \n",
    "由二阶展开后求导的性质，若是二次函数（只有一个极小点），牛顿法会直接跳到极小值，如果目标函数是非二次凸函数，则该更新将是迭代的，如下描述：   \n",
    "    <img src=\"img/niuton.png\" width=560 height=360>     \n",
    "\n",
    "之前讨论过，牛顿法只适用于与Hessian矩阵正定的情况，对于包含负曲率（如鞍点）时会有问题，为此可以在对角矩阵增加常数$\\alpha$引入正则化：    \n",
    "    &emsp;$\\theta^* = \\theta_0 - [H(f(\\theta_0)) + \\alpha I]^{-1}g$    \n",
    "这种处理方法也有问题，当负曲率过大时，$\\alpha$的值需要很大，变成对角矩阵主导，参数更新变成梯度除以$\\alpha$，会使得最终的步长变小。   \n",
    "此外，牛顿法用到的H矩阵中的元素数量是参数数量的平方，有较大的计算复杂度，其结果是只有参数很少的网络才能在实际中用牛顿法训练。    \n",
    "### 8.6.2 共轭梯度\n",
    "从最速下降法中启发，最速下降法每次下降的方向是正交的，可能导致某一方向上的梯度上一次优化的效果被这一次抵消，如下图：\n",
    "       <img src=\"img/zuisu.png\" width=240 height=240>     \n",
    "\n",
    "为此，使用共轭梯度，保证两次下降方向互不干扰，既满足条件：$d_t^T Hd_t-1=0$, 为此需要对$d_t$进行修正，$d_t = \\nabla_{\\theta}J(\\theta)+\\beta_t d_{t-1}$，需要求$\\beta_t$,有如下两种方式避免直接从H求解：    \n",
    "    <img src=\"img/beta.png\" width=560 height=480>\n",
    "具体的算法如下：    \n",
    "    <img src=\"img/conj_gradient.png\" width=560 height=480>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 优化策略和元算法    \n",
    "### 8.7.1 批标准化    \n",
    "设想对于每层只有一个单元的深度神经网络，每个隐藏层不使用激活函数：    \n",
    "&emsp; $ \\hat y = xw_1w_2w_3...w_l$      \n",
    "其中$w_l$用于表示第i层的权重，在使用标准梯度下降法(从一阶层面考虑优化问题）时，假设我们希望$\\hat y$下降0.1，那么学习率应该为$\\frac{0.1}{g^Tg}$,最终得到y的更新值为：     \n",
    "&emsp;$ x(w_1 - \\epsilon g_1)(w2-\\epsilon g_2)...(w_l - \\epsilon g_l)$      \n",
    "可以看到由于梯度下降法从一阶考虑优化问题，所以上式中多出的二阶及以上的高阶项会影响最终的优化效果，如二阶项$\\epsilon^2 g_1g_2\\prod_{i=3}^l w_i$,当$\\prod_{i=3}^l w_i$很小时，该项可以忽略不计，但是如果每个权重$w_i$都比1大时，便会对结果产生很大的影响，造成优化效果与预期的偏差，尤其当网络层数较深时，底层的网络参数变化对高层的隐层结果影响巨大，为了解决这个问题（该问题被原作者称为**Internal Convariance Shift**,[相关通俗解释](https://zhuanlan.zhihu.com/p/34879333))，即有批标准化，即对每个隐藏层的线性输出标准化，服从$N(0,1)$分布，从而使每一层的高阶项（其实这时只存在二阶项，因为上一层的二阶项已经被标准化可以忽略不记，不会传到当前层）都被约束，而不会造成底层的参数变化对高层有很大影响的情况：    \n",
    "&emsp; $ H' = \\frac{H-\\mu}{\\delta}$    \n",
    "需要注意，批标准化只是标准化隐层，即作用的最高层为$h_{l-1}$，输出层不能被标准化。此外，虽然批标准化使得模型容易学习，但是代价是使得底层网络没有用，当然，这只是针对线性变换，对于如神经网络每层激活函数之类的非线性变换，仍然不受影响。      \n",
    "此外，标准化会使得每一层网络的表示能力下降，因此将标准化后的隐藏单元$H'$替换为$\\gamma H' +\\beta$, 为什么之前将均值标准化为0，现在又将其设置为$\\beta$呢？首先因为这里的$\\gamma 和\\beta$都是可以动态学习的参数，其次标准化之前的均值与底层复杂关联，而这里的均值只与$\\beta $有关。      \n",
    "**但是通过做实验发现，单纯较深的全连接神经网络，BN没有效果，反而训练变慢，可能对较深的CNN才有效果**    \n",
    "### 8.7.2 监督预训练\n",
    "如果一个模型太复杂难以优化，通常先训练一个简单的模型求解问题，然后使模型更复杂会更有效，这类方法统称为**预训练**。    \n",
    "1. **贪心算法**将问题分解成许多部分，然后独立的在每个部分求解最优值，虽然不能保证得到最佳解，但高效，贪心算法也可以紧接**精调**阶段。    \n",
    "贪心监督预训练有效的理解为其对中间层的训练有指导作用。一般而言其对优化和泛化都是有帮助的。    \n",
    "2. **迁移学习**也是其扩展，先训练第一个网络，然后利用第一个网络的前k层联合第二个网络训练以执行不同的任务，但是训练样本少于第一个任务。    \n",
    "3. **FitNets**: 先训练足够宽足够浅的**老师网络**，然后用其指导**足够深足够窄的学生网络**，学生网络不仅需要预测原任务的输出，还需要在中间层预测教师网络中间层的值，这样便可以训练原本难以训练的又窄又深的网络，且又窄又深的网络泛化能力更好。     \n",
    "\n",
    "### 8.7.3 设计有助于优化的模型    \n",
    "相对于使用优化算法或者改进，更加重要的是设计有助于优化的模型，如：   \n",
    "1. 使用更多的线性函数（LSTM, RELU)，简化优化，使Jacobian具有相对合理的奇异值    \n",
    "2. 跳跃连接,减少底层参数到输出的长度，缓解梯度消失问题。如GoogleNet     \n",
    "\n",
    "### 8.7.4 延拓法和课程学习      \n",
    "**延拓法**：设计从简单到复杂的代价函数，逐步优化，其理念是非凸函数在模糊后近似凸。   \n",
    "**课程学习**： 先学习简单概念，然后逐步学习依赖于这些简单概念的复杂概念。     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
