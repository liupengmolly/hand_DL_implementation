{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7深度学习中的正则化\n",
    "## 7.1 参数范数惩罚\n",
    "### 7.1.1 $L^2$参数正则化\n",
    "对L2正则化，代价函数如下：    \n",
    "&emsp; $ \\tilde J(w;X,y) = \\frac{\\alpha}{2}w^Tw + J(w;X,y)$.   \n",
    "对应梯度为：&emsp; $ \\nabla_w \\tilde J(w;X,y) = \\alpha w + \\nabla_J(w;X,y)$    \n",
    "对应的梯度更新为：    \n",
    "&emsp; $w \\gets w - \\epsilon(\\alpha w + \\nabla_w J(w;X,y))$    \n",
    "下面进一步分析w在此情况优化的含义，假定$w^*$是不加正则化的最优解，利用泰勒展开(因为最优解，所以一阶导为0）：    \n",
    "&emsp; $\\hat J(\\theta) = J(w^*) + \\frac{1}{2}(w-w^*)^TH(w-w^*)$.         \n",
    "解得：&emsp; $\\nabla_w \\hat J(w) = H(w - w^*)$为0。       \n",
    "加入正则项：&emsp; $\\nabla_w \\hat J(w) = \\alpha \\tilde w + H(\\tilde w - w^*)=0$     \n",
    "解得： &emsp;$ \\tilde w = (H+\\alpha I)^{-1} H w^* $        \n",
    "此时引入一个重要的性质：**实对称矩阵（H）能分解为一个对角矩阵$\\Lambda$和一组特征向量的标准正交基Q**,   即$H=Q\\Lambda Q^T$,代入上式得：     \n",
    "&emsp; $ \\tilde w = Q(\\Lambda + \\alpha I)^{-1}\\Lambda Q^T w^* $.   \n",
    "由此能看到权重衰减的效果就是沿着H的特向量所定义的轴缩放$w^*$，即根据$\\frac{\\lambda_i}{\\lambda_i + \\alpha}$因子缩放与第i个特征向量所对应的$w^*$的分量，由此可以发现**特征值较大的方向缩减较小，特征值较小的方向缩减较大**，如下图所示：    \n",
    "    <img src=\"img/l2.png\" width=320 height=320>\n",
    "实线表示目标函数的等值线，虚线表示L2正则项的等值线，对于前者，梯度比较大的方向对对该方向的移动（参数的缩减）比较敏感，特征值较大（较小的参数变化就能有较大的影响，说明对应的变量变化范围较大，即该方向特征向量对应的特征值较大），而梯度较小的方向（横向）对该方向的移动没那么敏感，特征值较小，所以为了减小L2正则项，容易将该方向参数缩减为0。    \n",
    "\n",
    "### 7.1.2 $L^1$参数正则化    \n",
    "也能如7.1.1中通过泰勒展开到用转换为元素级表示通过l1正则化最终得到的优化参数的含义，具体见书。    \n",
    "L1正则化会产生更为稀疏的解，相对应的L2并不会稀疏，所以L1正则化通常广泛的用于特征选择（Lasso，在ESL中有详细的介绍以及与Lasso相关的若干特征选择方法）。     \n",
    "此外，从最大后验估计（MAP）角度解释，L1正则化相当于权重先验为各向同性的拉普拉斯分布的对数先验，拉普拉斯的分布如下：    \n",
    "&emsp; $ f(x) = \\frac{1}{2\\lambda} e^{-\\frac{|x-\\mu|}{\\lambda}}$    \n",
    "\n",
    "## 7.2 作为约束的范数惩罚      \n",
    "通过构建广义的Lagrange函数在目标函数上加上一系列约束惩罚想，每个惩罚是一个被称为KKT乘子的系数以及一个表示约束是否满足的函数之间的乘积：    \n",
    "&emsp; $\\zeta(\\theta,\\alpha;X,y) = J(\\theta;X,y)+\\alpha (\\Omega(\\theta)-k)$     \n",
    "当$\\Omega(\\theta)>k$时，$\\alpha$必须减小，当$\\Omega(\\theta)<k$时，$\\alpha$必须增加，此外，较大的$\\alpha$对应较小的约束区域，较小的$\\alpha$对应较大的约束区域。    \n",
    "\n",
    "## 7.3 正则化和欠约束问题    \n",
    "当数据分布在一些方向上没有差异，或者数据样本过少导致在一些方向上没有方差时，该输入矩阵就是奇异的，对应没有闭式解，可能是欠定的，对于一个欠定问题，可能存在多个解，如w是完美解，那么2w,3w,...都会是完美解，如此造成数值上溢，所以，我们加入正则化，$X^TX + \\alpha I$对应求逆，即伪逆：    \n",
    "&emsp;$ X^+ = \\lim \\limits_{\\alpha \\searrow 0}(X^TX + \\alpha I)^{-1}X^T$     \n",
    "\n",
    "## 7.4 数据集增强。  \n",
    "图像中的平移、旋转、缩放，语音的加入噪声等。    \n",
    "\n",
    "## 7.5 噪声鲁棒性    \n",
    "向输入中添加极小的噪声等价于对权重施加范数惩罚，此外还有向隐藏层加入噪声（dropout）以及将噪声加入到权重（循环神经网络）。    \n",
    "例如在输入中加入随机扰动$\\epsilon_w \\sim N(\\epsilon;0,\\eta I)$后，最小化带权重噪声（方差为$\\eta I$)的J等同于最小化附加正则化项，这种形式的正则化鼓励参数进入小的权重扰动对输出影响相对较小的参数空间区域，即** 找到的点不仅是极小点，还是由平坦区域所包围的极小点**     \n",
    "** 向输出目标中注入噪声**：由于大多数数据集y标签是有一定误差的，如果明确目标（即y非0即1），那么用softmax函数进行最大似然学习时永远不会收敛，为防止这种情况，使用标签平滑，即将分类目标从0和1替换程$\\frac{\\epsilon}{k-1}$和$1-\\epsilon$, 从而防止模型追求确切概率。     \n",
    "## 7.6 多任务学习    \n",
    "通过合并几个任务中的样例（可以视为对参数施加的软约束）来提高繁华的一种方式，当模型的一部分被多个额外的任务共享时，这部分将被约束为良好的值。于是一个模型可以分为两类参数:    \n",
    "(1) 具体任务的参数    \n",
    "(2) 所有任务共享的通用参数     \n",
    "## 7.7提前终止     \n",
    "提前终止的额外代价：    \n",
    "    * 要定期评估验证集\n",
    "    * 需要保持最佳的参数副本     \n",
    "提前终止需要验证集，这意味着某些训练数据不能被模型学习，为此，我们可以在提前终止后再次训练，具体有两种策略。    \n",
    "1. 利用第一轮提前终止确定的步数i,重新在全部的数据上训练模型（参数重置），直到i步\n",
    "2. 利用第一轮训练的参数，继续在全部数据上训练，直到在验证集上的平均损失低于提前终止时的损失（会有数据泄露？）  \n",
    "\n",
    "### 7.7.1提前终止为何具有正则化效果\n",
    "提前终止和L2正则化很相似，如下图，左边的$\\tilde w$表示提前终止在目标函数的位置，没有接近最低点，但是离原点较近，和右图有一定约束范围的的参数空间于目标函数相交的点一样。    \n",
    "    <img src=\"img/earlier.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 参数共享\n",
    "除了使用参数范数惩罚正则化参数外，还有的做法是强迫某些参数相等，即**参数共享**，例如卷积神经网络。    \n",
    "\n",
    "## 7.9 Bagging和其他集成方法   \n",
    "Bagging是通过结合几个模型降低泛化误差的技术，也被称为**模型平均**。     \n",
    "假设有k个回归模型，每个模型的误差是$\\epsilon_i$,服从零均值方差$E[\\epsilon_i^2] = v$且协方差为$E[\\epsilon_i\\epsilon_j] = c$的多维正态分布，通过所有即成模型的平均误差是$\\frac{1}{k} \\sum_i \\epsilon_i$，平方误差的期望是：    \n",
    "&emsp; $ E[(\\frac{1}{k} \\sum \\limits_i \\epsilon_i)^2] = \\frac{1}{k^2} E[\\sum\\limits_i (\\epsilon_i^2 + \\sum\\limits_{j\\ne i}\\epsilon_i\\epsilon_j)]$    \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $=\\frac{1}{k} v + \\frac{k-1}{k} c$.    \n",
    "当各模型完全相关， 即$c=v$的情况下，均方误差为v，当模型完全无关，即$c=0$时，均方误差为$\\frac{1}{k}v$    \n",
    "通常各模型可以使用不同的算法或者目标函数训练得到，也可以构造k个不同的数据集，每个数据集从原始数据集重复采样得到（如训练集若与原始数据集大小相同，通常包含原始数据集2/3的数据）。  \n",
    "## 7.10 Dropout     \n",
    "DropOut某种程度上类似于Bagging,都可以看成对多个模型的集成，不过Bagging中的多个模型往往是不相关分别训练的，而DropOut中的多个模型可以看成有一个共同的父模型，每一步训练中选取其中的部分参数训练，即训练子模型，且不同子模型之间是串行训练的，最终预测时把所有训练的子模型合起来，即完整的父模型。    \n",
    "**补充**：算术平均是对分布的平均，几何平均是对数的累积求n次根，更多是对首尾连续的比例的平均（如连续几年的股票收益率）。     \n",
    "DropOut的优点：     \n",
    "    * 计算简便。  \n",
    "    * 对适用的模型或训练过程没有限制。  \n",
    "DropOut的缺点：     \n",
    "尽管dropout在一步上的计算代价很小，但是由于减少了参数，即较少了模型的有效容量，为抵消这种影响，必须增大模型规模，同时会增加训练的时间，尤其在数据集较大时，使用dropout代价太大。     \n",
    "Dropout的另一个说法是不同模型共享隐藏单元，且为了在不同模型之间的交换，这些隐藏单元必须稳定且表现良好。     \n",
    "## 7.11 对抗训练     \n",
    "在精度达到人类水平的神经网络上通过优化过程故意构造数据点，模型的误差率会接近100%，称为**对抗样本**，人类往往不易察觉到原始样本与对抗样本的差异，但网路会给出非常不同的预测。     \n",
    "对抗样本的主要原因是**过度线性**，对抗训练鼓励网络在训练数据附近的局部区域恒定来限制这一高度敏感的局部线性行为，可以看作引入局部恒定先验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
