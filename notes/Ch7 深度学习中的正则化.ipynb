{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7深度学习中的正则化\n",
    "## 7.1 参数范数惩罚\n",
    "### 7.1.1 $L^2$参数正则化\n",
    "对L2正则化，代价函数如下：    \n",
    "&emsp; $ \\tilde J(w;X,y) = \\frac{\\alpha}{2}w^Tw + J(w;X,y)$.   \n",
    "对应梯度为：&emsp; $ \\nabla_w \\tilde J(w;X,y) = \\alpha w + \\nabla_J(w;X,y)$    \n",
    "对应的梯度更新为：    \n",
    "&emsp; $w \\gets w - \\epsilon(\\alpha w + \\nabla_w J(w;X,y))$    \n",
    "下面进一步分析w在此情况优化的含义，假定$w^*$是不加正则化的最优解，利用泰勒展开(因为最优解，所以一阶导为0）：    \n",
    "&emsp; $\\hat J(\\theta) = J(w^*) + \\frac{1}{2}(w-w^*)^TH(w-w^*)$.         \n",
    "解得：&emsp; $\\nabla_w \\hat J(w) = H(w - w^*)$为0。       \n",
    "加入正则项：&emsp; $\\nabla_w \\hat J(w) = \\alpha \\tilde w + H(\\tilde w - w^*)=0$     \n",
    "解得： &emsp;$ \\tilde w = (H+\\alpha I)^{-1} H w^* $\n",
    "此时引入一个重要的性质：**实对称矩阵（H）能分解为一个对角矩阵$\\Lambda$和一组特征向量的标准正交基Q**,   即$H=Q\\Lambda Q^T$,代入上式得：     \n",
    "&emsp; $ \\tilde w = Q(\\Lambda + \\alpha I)^{-1}\\Lambda Q^T w^* $.   \n",
    "由此能看到权重衰减的效果就是沿着H的特向量所定义的轴缩放$w^*$，即根据$\\frac{\\lambda_i}{\\lambda_i + \\alpha}$因子缩放与第i个特征向量所对应的$w^*$的分量，由此可以发现**特征值较大的方向缩减较小，特征值较小的方向缩减较大**，如下图所示：    \n",
    "    <img src=\"img/l2.png\" width=320 height=320>\n",
    "实线表示目标函数的等值线，虚线表示L2正则项的等值线，对于前者，梯度比较大的方向对对该方向的移动（参数的缩减）比较敏感，特征值较大（较小的参数变化就能有较大的影响，说明对应的变量变化范围较大，即该方向特征向量对应的特征值较大），而梯度较小的方向（横向）对该方向的移动没那么敏感，特征值较小，所以为了减小L2正则项，容易将该方向参数缩减为0。    \n",
    "\n",
    "### 7.1.2 $L^1$参数正则化    \n",
    "也能如7.1.1中通过泰勒展开到用转换为元素级表示通过l1正则化最终得到的优化参数的含义，具体见书。    \n",
    "L1正则化会产生更为稀疏的解，相对应的L2并不会稀疏，所以L1正则化通常广泛的用于特征选择（Lasso，在ESL中有详细的介绍以及与Lasso相关的若干特征选择方法）。     \n",
    "此外，从最大后验估计（MAP）角度解释，L1正则化相当于权重先验为各向同性的拉普拉斯分布的对数先验，拉普拉斯的分布如下：    \n",
    "&emsp; $ f(x) = \\frac{1}{2\\lambda} e^{-\\frac{|x-\\mu|}{\\lambda}}$    \n",
    "\n",
    "## 7.2 作为约束的范数惩罚      \n",
    "通过构建广义的Lagrange函数在目标函数上加上一系列约束惩罚想，每个惩罚是一个被称为KKT乘子的系数以及一个表示约束是否满足的函数之间的乘积：    \n",
    "&emsp; $\\zeta(\\theta,\\alpha;X,y) = J(\\theta;X,y)+\\alpha (\\Omega(\\theta)-k)$     \n",
    "当$\\Omega(\\theta)>k$时，$\\alpha$必须减小，当$\\Omega(\\theta)<k$时，$\\alpha$必须增加，此外，较大的$\\alpha$对应较小的约束区域，较小的$\\alpha$对应较大的约束区域。    \n",
    "\n",
    "## 7.3 正则化和欠约束问题    \n",
    "当数据分布在一些方向上没有差异，或者数据样本过少导致在一些方向上没有方差时，该输入矩阵就是奇异的，对应没有闭式解，可能是欠定的，对于一个欠定问题，可能存在多个解，如w是完美解，那么2w,3w,...都会是完美解，如此造成数值上溢，所以，我们加入正则化，$X^TX + \\alpha I$对应求逆，即伪逆：    \n",
    "&emsp;$ X^+ = \\lim \\limits_{\\alpha \\searrow 0}(X^TX + \\alpha I)^{-1}X^T$     \n",
    "\n",
    "## 7.4 数据集增强。  \n",
    "图像中的平移、旋转、缩放，语音的加入噪声等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
