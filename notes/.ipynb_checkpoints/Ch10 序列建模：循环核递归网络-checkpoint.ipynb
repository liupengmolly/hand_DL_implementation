{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**从多层网络出发到循环网络，利用到20世纪80年代机器学习和统计模型早期思想的优点：在模型不同部分共享参数，参数共享使得模型能扩展到不同形式（不同长度）的样本并泛化，如果每个时间点都有一个单独参数，不但不能泛化到任意长度，也不能在时间上共享不同序列长度和不同位置的统计强度**。  \n",
    "# 10.1 循环神经网络       \n",
    "循环神经网络包含以下几种重要的设计模式：    \n",
    "1. 每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络,如下图:\n",
    "    <img src=\"img/RNN_mode_1.png\" height=240 width=360>      \n",
    "   其前向传播公式为：    \n",
    "   &emsp; $a^{(t)} = b + Wh^{(t-1)}+Ux^{(t)}$.    \n",
    "   &emsp; $h^{(t)} = tanh(a^{(t)})$.    \n",
    "   &emsp; $o^{(t)} = c + Vh^{(t)}$.     \n",
    "   &emsp; $y^{(t)} = softmax(o^{(t)})$.     \n",
    "   能够发现，该模型需严格的从左往右进行计算，其对应的反向传播算法称为**通过时间反向传播（BPTT)**.    \n",
    "2. 每个时间步都产生输出，只有当前时刻的输出到下一时刻的隐藏单元之间有循环连接的循环网络(其表示能力不如1，因为O是输出层的维度大小，存储信息的能力变弱）：\n",
    "    <img src=\"img/RNN_mode_2.png\" height=240 width=360>      \n",
    "    该模型属于**导师驱动过程**，各时间步解耦了，所以可以并行化，即在训练时用正确的输出$y^t$反馈到$h^{(t+1)}$，测试时，用模型的输出$o^t$近似正确的输出$y^t$作为下一层的输入。    \n",
    "3. 读取整个序列后产生单个输出，隐藏单元之间存在循环连接的循环网络：    \n",
    "    <img src=\"img/RNN_mode_3.png\" height=240 width=360>     \n",
    "4. 基于上下文的RNN序列建模,该序列的本质是向量到序列，在S2S的解码层会用到。   \n",
    "    <img src=\"img/RNN_mode4.png\" height=240 width=360>.      \n",
    "\n",
    "# 10.2 双向RNN.     \n",
    "许多应用中，我们要输出的$y^t$的预测可能依赖于整个输入序列，包括未来的，而不仅仅是之前的，所以就有双向RNN,结合从序列起点开始移动的RNN和另一个从序列尾部开始的RNN.    \n",
    "# 10.3 基于编码-解码的序列到序列架构     \n",
    "用于输入与输出的长度不一致的情况，广泛应用于机器翻译、语音识别等领域：     \n",
    "    <img src=\"img/RNN_S2S.png\" height=240 width=360>.   \n",
    "由于C的维度太小，很难产生有效的序列，为此，可以将c替换成可变长度的序列，还有引入注意力机制。    \n",
    "# 10.4 递归神经网络      \n",
    "递归神经网络不同于循环神经网络使用链式结构，其使用树状结构，其明显的优势是对于长度为t的序列，深度可以急剧的从t减小为O(log t),这可能有助于解决长期依赖，但是悬而未决的问题是如何构造树，有两种方法，一种是使用不依赖于数据的树结构，如平衡二叉树，一种是使用外部的树结构作为借鉴，如自然语言句子的句法分析的树结构。    \n",
    "# 10.5 长期依赖的挑战       \n",
    "$h^{(t)} = W^T h^{(t-1)}$       \n",
    "$h^{(t)} = (W^t)^T h^(0)$      \n",
    "当W符合特征分解时： $W = Q \\Lambda Q^T$     \n",
    "其中Q正交，那么: $h^{(t)} = Q^T \\Lambda^t Q h^{(0)}$    \n",
    "其中$\\Lambda$表示对应特征向量方向的特征值，当$\\Lambda$大于1时产生梯度爆炸问题，小于1时，产生梯度弥散问题。     \n",
    "# 10.6  渗漏单元和跳跃连接       \n",
    "1. 渗漏单元：将某些v值应用更新：$\\mu^{(t)} \\gets \\alpha \\mu^{(t-1)} + (1-\\alpha)v^{(t)}$累积一个过去的滑动平均值$\\mu^{(t)}$    \n",
    "2. 跳跃连接：设计工作在多个时间尺度的模型，如跳跃d步，那么能获得更远处粗粒度的信息，其导数的减小速度为$\\frac{tau}{d}$而不是$\\tau$。     \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
