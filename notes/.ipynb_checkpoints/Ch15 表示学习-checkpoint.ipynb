{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务处理的难度，直接与信息如何表示有关**。       \n",
    "在机器学习中，选择什么表示通常与后续的任务有关，对于监督巡练中的前馈网络，可以将除了最后一层的softmax分类起器视为线性分类器外，其余的部分都可以视为对输入的一种表示学习，将输入从线性不可分变成线性可分。       \n",
    "一般表示学习除了更多的学习如何保存并表示数据的属性外，还会追求数据本身良好的性质，如独立性等，所以大部分表示学习算法都是在**保留与输入相关的信息与追求良好的性质之间平衡**。      \n",
    "此外，表示学习提供了进行无监督和半监督的一种方法，能从**未标注数据上学习出很好的表示，然后用这些表示解决监督学习问题**。      \n",
    "# 15.1 贪心逐层无监督训练         \n",
    "   <img src=\"img/greedy_unsupervised.png\" height=560 width=560>      \n",
    "   \n",
    "其中$\\zeta$表示无监督学习算法。       \n",
    "无监督训练结合两种不同的思想：     \n",
    "1. ** 有显著的正则化效果**     \n",
    "这通常在数据集较小的情况下有效，神经网络对数据预测的不确定性，数据规模较小时尤其明显，并且这种情况下往往会造成输入矩阵的病态条件等情况，所以一方面网络难以找到较优值，另一方面网络容易有较大的方差，而使用无监督预训练，减少了这种不确定性，把网络输出引向一个固定的区域，减少方差，且效果也不至于很差。      \n",
    "2. ** 学习输入分布，有助于监督学习**。   \n",
    "这通常也是在初始表示较差的情况下，如自然语言处理的one-hot向量，任意向量之间的距离都是1，利用无监督预训练后，能学习到更多的信息。然而，在图像处理时不太有用，因为图像已经在一个很丰富的向量空间中。     \n",
    "\n",
    "此外，无监督学习也有一些问题，因为无监督预训练与监督学习都有各自的超参数，而它们不能同时调整，各自的效果不能彼此借鉴，此外无监督预训练并不提供调节正则化强度的超参数，所以不能灵活调整。     \n",
    "\n",
    "**最后，大部分的无监督预训练研究都是在现有的优秀监督神经网络流行之前，现在大部分算法已经不用无监督预训练了，但是其思想仍有很借鉴性。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
