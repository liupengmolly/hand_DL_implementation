{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.1 卷积运算\n",
    "通过一个例子，为了对信号噪声平滑，追踪传感器对飞船在时刻t的位置由之前若干位置加权平均：$s(t) = \\int x(a)w(t-a)da$,该函数操作即为**卷积**，通常第一个参数x叫做**输入**，第二个参数w叫做**核函数**。    \n",
    "也可以在二维上卷积：    \n",
    "&emsp;$S(i,j) = (I*K)(i,j) = \\sum\\limits_m \\sum\\limits_n I(m,n)K(i-m,j-n)$.   \n",
    "卷积可以交换，上式等价为：    \n",
    "&emsp;$S(i,j) = (K*I)(i,j) = \\sum\\limits_m\\sum\\limits_n I(i-m,j-n)K(m,n)$.       \n",
    "可交换性在神经网络中并不是一个重要的性质,反而上式减号所代表的**翻转**值得注意，因为大部分神经网络库的卷积实现并没有进行翻转，即为：    \n",
    "&emsp; $ S(i,j) = (I*K)(i,j) = \\sum\\limits_m\\sum\\limits_n I(i+m,j+n)K(m,n)$     \n",
    "# 9.2 动机     \n",
    "1. 稀疏交互：核的大小远小于输入的大小。  \n",
    "2. 参数共享：核会被使用多次，用在输入的所有位置。    \n",
    "3. 等变：参数共享使得神经网络具有平移等变的性质，即一个函数f对输入改变，输出也以同样方式改变，即假设g函数表示卷积操作，则f(g(x)) = g(f(x)),那么f函数对于卷积就有等变性。     \n",
    "    此外，卷积中的一些其他元素不是天然等变的，如缩放和旋转，需要其他机制处理。    \n",
    "\n",
    "# 9.3 池化     \n",
    "** 池化是使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出**。     \n",
    "当输入作出少量平移时，池化能帮助输入的表示近似不变，这对于我们关心某个特征是否出现而不关心其出现位置时很有用。    \n",
    "所以池化可以看做一个无限强的**先验**：该层雪的函数必须具有对少量平移的不变性。    \n",
    "池化是一种降采样，能够减小下一层的输入规模，某种程度上减少参数，     \n",
    "# 9.4 卷积与池化作为一种无限强的先验。  \n",
    "可以把卷积的使用当作在全连接层中学得的函数只包含局部连接关系并对平移由等变性，类似的池化也是无限强的先验：每一个单元都具有对少量平移的不变性。    \n",
    "当然，把卷积网络当作具有无限强先验的全连接网络会导致极大的计算资源浪费，但可以帮助我们洞察卷积网络是如何工作的，例如卷积和池化有时会导致欠拟合的原因是其先验假设不满足任务要求。        \n",
    "\n",
    "# 9.5 基本卷积函数的变体     \n",
    "在神经网络上下文中讨论卷积时，通常与数学文献中的离散卷积不同。如我们会使用并行卷积实现多通道，每个通道表示一种特征提取器。     \n",
    "1. 基本卷积网络：    \n",
    "    假定有一个4维的核张量K，每一个元素$K_{i,j,k,l}$表示输出中处于通道i的一个单元和输入中处于通道j的一个单元的连接强度，并且输出单元与输入单元之间有k行l列的偏置，假定观测数据V，其每一个元素$V_{i,j,k}$表示处在通道i中第j行k列的值，则：     \n",
    "    &emsp; $ Z_{i,j,k} = \\sum\\limits_{l,m,n} V_{l,j+m-1,k+n-1}K_{i,l,m,n}$.   \n",
    "    其中向量的索引从1开始。      \n",
    "2. 跳过核中的一些位置降低计算开销：    \n",
    "&emsp; $Z_{i,j,k} = c(K,V,s)_{i,j,k} = \\sum\\limits_{l,m,n}[V_{l,(j-1)xs+m,(k-1)xs+n}, K_{i,l,m,n}].    \n",
    "    相当于单位步幅的卷积后将采样，但后者在计算上是浪费的，因为降采样之前计算了许多将被丢弃的值。      \n",
    "\n",
    "**关于零填充**：有**有效卷积**，**相同卷积**，**全卷积**三种方式，一般零填充的最优数量处于有效卷积核相同卷积之间的某个位置。     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.6 结构化输出。  \n",
    "我们可以通过卷积最终对每个图像的像素进行分类，其中一个问题是输出与输入的大小不一致，对此可以取消下采样的操作或者实现上采样，也可以利用低分辨率的标签。      \n",
    "一旦对每个像素都进行了预测，就可以获得图像在区域上的分割以及用于其他任务。     \n",
    "\n",
    "**卷积可以处理可变尺寸的输入，但是一般是因为包含对同种事物不同量的观察而导致的尺寸变化才有意义，如果输入是因为可以选择性的包含不同种类的观察而具有的可变尺寸时，使用卷积是不合理的。**     \n",
    "# 9.7 随机或无监督的特征      \n",
    "卷积网络训练中较为昂贵的即为学习特征，减少网络训练成本的一种方法是不使用监督方式训练而得到特征。有三种基本策略：    \n",
    "* 随机初始化\n",
    "* 手动设计，例如设置每个核在特定方向或尺度来检测边缘。  \n",
    "* 使用无监督的标准来学习，如将k均值用于小图像模块，然后使用每个学得中心作为卷积核。     \n",
    "\n",
    "# 9.8 卷积网络的神经科学\n",
    "卷机网络是将研究大脑获得的深刻理解成功用于机器学习的关键例子。    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
