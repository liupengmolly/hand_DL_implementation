{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 机器学习基础\n",
    "## 5.1 最大似然估计\n",
    "1. 考虑一组含有m个样本的数据集$X=\\{x^{(1)},...,x^{(m)}\\}$,对$\\theta$的最大似然估计为：    \n",
    "&emsp; $\\Theta_{ML} = arg \\max\\limits_{\\Theta}\\prod\\limits_{i=1}^m p_{model} (x^{(i)};\\Theta) $ &emsp; (1)    \n",
    "等价于：  $\\Theta_{ML} = arg \\max\\limits_{\\Theta}\\sum\\limits_{i=1}^m log\\ p_{model} (x^{(i)};\\Theta) $ &emsp; (2)    \n",
    "将上式除以m得： $\\Theta_{ML} = arg \\max\\limits_{\\Theta}E_{x\\sim \\hat p_{data}}\\ log\\ p_{model} (x^{(i)};\\Theta) $ &emsp; (3)    \n",
    "一种解释最大似然估计的观点是将它看作最小化训练集上的经验分布${\\hat p}_data$与模型分布之间的差异，用KL散度度量：    \n",
    "&emsp; $D_{KL}({\\hat p}_{data}\\parallel p_{model}) = E_{x\\sim \\hat p_{data}}\\ [log \\hat{p}_{data}(x) - logp_{model}(x)]$    &emsp;(4)    \n",
    "等式前项只与数据的采集方式有关，最小化KL散度，等价于最小化：    \n",
    "&emsp; $-E_{x\\sim \\hat p_{data}}\\ logp_{model}(x)$    &emsp;(5)       \n",
    "也就是**交叉熵**，可以看出任何一个负对数似然组成损失都是定义在训练集上的经验分布与定义在模型上的概率分布的交叉熵。    \n",
    "&nbsp;\n",
    "2. **条件对数似然和均方误差**    \n",
    "最大似然估计很容易扩展到最大条件对数似然：    \n",
    "&emsp;$\\Theta_{ML} = arg \\max\\limits_{\\Theta}\\sum\\limits_{i=1}^m log\\ P\\ (y^{(i)}\\ |\\ x^{(i)};\\Theta) $ &emsp; (6)     \n",
    "然后可以用最大对数似然得到从输入x映射到y的模型，当假设p满足高斯分布时，可以推导出和均方误差一样的结果。    \n",
    "&emsp; $\\sum\\limits_{i=1}^m log\\ P\\ (y^{(i)}\\ |\\ x^{(i)};\\theta) $     \n",
    "$=\\sum\\limits_{i=1}^m log\\ N(y^{(i)};\\hat{y}(x;\\ theta),\\delta^2)$    \n",
    "$=-mlog\\delta - \\frac{m}{2}log(2\\pi) - \\sum\\limits_{i=1}^{m} \\frac{\\lVert \\hat{y}^{(i)} - y^{(i)} \\rVert^2}{2\\delta^2}$    &emsp; (7).   \n",
    "明显可以看出最大化关于$\\theta$的对数似然和最小化均方误差会得到相同的参数估计。    \n",
    "\n",
    "## 5.2 贝叶斯统计\n",
    "最大似然估计属于频率派方法，参数$\\theta$未知，直接由数据分布决定，概率表示数据的已知状态，而贝叶斯统计用概率反应知识的确定程度，在最开始，已知知识通常表示为先验概率分布，通常是相对均匀的分布和高熵的高斯分布。    \n",
    "1. 贝叶斯估计与最大似然估计的两个重要区别：    \n",
    "    * 最大似然方法预测时使用$\\theta$的点估计，而贝叶斯方法使用$\\theta$的全分布，如：    \n",
    "&emsp; $ p(x^{(m+1)}\\ |\\ x^{(1)},...,x^{(m)})\\ =\\ \\int p(x^{(m+1)}|\\theta)\\ p(\\theta|x^{(1)},...,x^{(m)})\\ d\\theta$&emsp;(8).    \n",
    "可以看到贝叶斯用积分利用每一个具有正概率密度的$\\theta$值，往往会防止过拟合。    \n",
    "    * 贝叶斯先验分布。    \n",
    "2. 当训练数据有限时，贝叶斯方法通常泛化得更好，但是当训练样本数目很大时，会有很大计算代价？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
