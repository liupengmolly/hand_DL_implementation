{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 机器学习基础\n",
    "## 5.1 最大似然估计\n",
    "1. 考虑一组含有m个样本的数据集$X=\\{x^{(1)},...,x^{(m)}\\}$,对$\\theta$的最大似然估计为：    \n",
    "&emsp; $\\Theta_{ML} = arg \\max\\limits_{\\Theta}\\prod\\limits_{i=1}^m p_{model} (x^{(i)};\\Theta) $ &emsp; (1)    \n",
    "等价于：  $\\Theta_{ML} = arg \\max\\limits_{\\Theta}\\sum\\limits_{i=1}^m log\\ p_{model} (x^{(i)};\\Theta) $ &emsp; (2)    \n",
    "将上式除以m得： $\\Theta_{ML} = arg \\max\\limits_{\\Theta}E_{x\\sim \\hat p_{data}}\\ log\\ p_{model} (x^{(i)};\\Theta) $ &emsp; (3)    \n",
    "一种解释最大似然估计的观点是将它看作最小化训练集上的经验分布${\\hat p}_data$与模型分布之间的差异，用KL散度度量：    \n",
    "&emsp; $D_{KL}({\\hat p}_{data}\\parallel p_{model}) = E_{x\\sim \\hat p_{data}}\\ [log \\hat{p}_{data}(x) - logp_{model}(x)]$    &emsp;(4)    \n",
    "等式前项只与数据的采集方式有关，最小化KL散度，等价于最小化：    \n",
    "&emsp; $-E_{x\\sim \\hat p_{data}}\\ logp_{model}(x)$    &emsp;(5)       \n",
    "也就是**交叉熵**，可以看出任何一个负对数似然组成损失都是定义在训练集上的经验分布与定义在模型上的概率分布的交叉熵。    \n",
    "&nbsp;\n",
    "2. **条件对数似然和均方误差**    \n",
    "最大似然估计很容易扩展到最大条件对数似然：    \n",
    "&emsp;$\\Theta_{ML} = arg \\max\\limits_{\\Theta}\\sum\\limits_{i=1}^m log\\ P\\ (y^{(i)}\\ |\\ x^{(i)};\\Theta) $ &emsp; (6)     \n",
    "然后可以用最大对数似然得到从输入x映射到y的模型，当假设p满足高斯分布时，可以推导出和均方误差一样的结果。    \n",
    "&emsp; $\\sum\\limits_{i=1}^m log\\ P\\ (y^{(i)}\\ |\\ x^{(i)};\\theta) $     \n",
    "$=\\sum\\limits_{i=1}^m log\\ N(y^{(i)};\\hat{y}(x;\\ theta),\\delta^2)$    \n",
    "$=-mlog\\delta - \\frac{m}{2}log(2\\pi) - \\sum\\limits_{i=1}^{m} \\frac{\\lVert \\hat{y}^{(i)} - y^{(i)} \\rVert^2}{2\\delta^2}$    &emsp; (7).   \n",
    "明显可以看出最大化关于$\\theta$的对数似然和最小化均方误差会得到相同的参数估计。    \n",
    "\n",
    "## 5.2 贝叶斯统计\n",
    "最大似然估计属于频率派方法，参数$\\theta$未知，直接由数据分布决定，概率表示数据的已知状态，而贝叶斯统计用概率反应知识的确定程度，在最开始，已知知识通常表示为先验概率分布，通常是相对均匀的分布和高熵的高斯分布。    \n",
    "1. 贝叶斯估计与最大似然估计的两个重要区别：    \n",
    "    * 最大似然方法预测时使用$\\theta$的点估计，而贝叶斯方法使用$\\theta$的全分布，如：    \n",
    "&emsp; $ p(x^{(m+1)}\\ |\\ x^{(1)},...,x^{(m)})\\ =\\ \\int p(x^{(m+1)}|\\theta)\\ p(\\theta|x^{(1)},...,x^{(m)})\\ d\\theta$&emsp;(8).    \n",
    "可以看到贝叶斯用积分利用每一个具有正概率密度的$\\theta$值，往往会防止过拟合。    \n",
    "    * 贝叶斯先验分布。    \n",
    "&nbsp;\n",
    "2. 当训练数据有限时，贝叶斯方法通常泛化得更好，但是当训练样本数目很大时，会有很大计算代价。    \n",
    "&nbsp;\n",
    "3. 贝叶斯在确定参数后验分布中的意义     \n",
    "给定一组m个训练样本$(X^{(train)}, y^{(train)})$，可以将对y的预测表示为$y^{(train)}$上的高斯分布：    \n",
    "&emsp; $p(y^{(train)}\\ |\\ X^{(train)},w) = N(y^{(train)};X^{(train)}w,\\ I)$    \n",
    "但为了确定模型参数w的后验分布，我们还需要指定一个先验分布，通常使用高斯分布作为先验分布：    \n",
    "&emsp; $p(w) = N(w;\\mu, \\Lambda_0)$    \n",
    "然后可得: $p(w|X,y) \\propto p(y|X,w)p(w)$（省略积分）    \n",
    "大多数情况下贝叶斯先验分布的直觉认为$\\mu_0=0$, 而如果设置$\\Lambda_0 = \\frac{1}{\\alpha}I$，那么$\\mu_m$对w的估计就和频率派带权重参数衰减惩罚$\\alpha w^Tw$的线性估计一样（协方差矩阵理解为w不同值的可能范围）。一个区别是，当$\\alpha$为0时，贝叶斯估计是未定义的，贝叶斯学习过程初始化为一个无限宽的w先验。    \n",
    "&nbsp;\n",
    "4. 最大后验概率估计    \n",
    "由于2中所提到的，设计贝叶斯后验的计算时非常棘手的，所以通常点估计也是需要的，即**最大后验估计**，通过先验影响点估计的选择来利用贝叶斯方法的优点。     \n",
    "&emsp; $\\theta_{MAP} = arg \\max \\limits_{\\theta} p(\\theta| x)= arg \\max \\limits_{\\theta}\\ log\\ p(x|\\theta) + log\\ p(\\theta)$&emsp;(9)    \n",
    "上式可以理解为对参数使用点估计（因为用贝叶斯后验参数的估计始终在变化，不稳定，不利于计算），只是这个点估计仍然是后验概率，所以最后分解为对数似然和对数先验两部分。    \n",
    "再考虑对数先验项，若使用具有高斯先验权重w的线性回归模型，如果先验是$N(w;0, \\frac{1}{\\lambda} I^2)$,那么式(9)的对数先验项正比于熟悉的权重衰减$\\lambda w^Tw$，这也正是贝叶斯推断的优势，能够利用来自先验的信息，这些信息无法从训练数据中获得，但有利于减少点估计的方差，当然，代价是增加了偏差。    \n",
    "很多正规化估计方法的可以被解释为贝叶斯推断的近似，其正则化附加项对应着$logp(\\theta)$，但不是所有都对应着MAP贝叶斯推断，有些正则化项并不是一个概率分布。  \n",
    "\n",
    "## 5.3 监督学习算法    \n",
    "1. 支持向量机     \n",
    "支持向量机的重要创新是**核技巧**，其强大的原因有两个：    \n",
    "    * 能够使用保证有效收敛的凸优化技术学习非线性模型。  \n",
    "    * 核函数的实现方法通常比直接构建$\\psi(x)$再算点积高效得多。   \n",
    "核技巧的一个缺点是计算决策函数的成本关于训练样本的数目是线性的，为了缓和这个缺点，支持向量机学习主要包含0的向量$\\alpha$, 那么判断新样本的类别仅需要计算非零$\\alpha_i$对应的训练样本的核函数，也就是**支持向量**。    \n",
    "此外，现代深度学习的设计旨在克服核技巧的这些限制。    \n",
    "2. k近邻算法能达到非常高的容量，当训练样本数目无穷大时，1-最近邻收敛到两倍贝叶斯误差，超过贝叶斯误差的原因是当训练样本无穷其随机的从距离为0的无穷点挑一个，如果我们使用临近点投票的方式，即会收敛到贝叶斯误差。？另外一个缺点是不能学习出哪一个特征更具有识别力。\n",
    "\n",
    "## 5.4 无监督学习算法\n",
    "1. 比起one-hot编码只能获取类别信息，而不能活去类别内部和类别之间的属性的相关性，分布式表示更可取。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
