{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7深度学习中的正则化\n",
    "## 7.1 参数范数惩罚\n",
    "### 7.1.1 $L^2$参数正则化\n",
    "对L2正则化，代价函数如下：    \n",
    "&emsp; $ \\tilde J(w;X,y) = \\frac{\\alpha}{2}w^Tw + J(w;X,y)$.   \n",
    "对应梯度为：&emsp; $ \\nabla_w \\tilde J(w;X,y) = \\alpha w + \\nabla_J(w;X,y)$    \n",
    "对应的梯度更新为：    \n",
    "&emsp; $w \\gets w - \\epsilon(\\alpha w + \\nabla_w J(w;X,y))$    \n",
    "下面进一步分析w在此情况优化的含义，假定$w^*$是不加正则化的最优解，利用泰勒展开(因为最优解，所以一阶导为0）：    \n",
    "&emsp; $\\hat J(\\theta) = J(w^*) + \\frac{1}{2}(w-w^*)^TH(w-w^*)$.         \n",
    "解得：&emsp; $\\nabla_w \\hat J(w) = H(w - w^*)$为0。       \n",
    "加入正则项：&emsp; $\\nabla_w \\hat J(w) = \\alpha \\tilde w + H(\\tilde w - w^*)=0$     \n",
    "解得： &emsp;$ \\tilde w = (H+\\alpha I)^{-1} H w^* $\n",
    "此时引入一个重要的性质：**实对称矩阵（H）能分解为一个对角矩阵$\\Lambda$和一组特征向量的标准正交基Q**,   即$H=Q\\Lambda Q^T$,代入上式得：     \n",
    "&emsp; $ \\tilde w = Q(\\Lambda + \\alpha I)^{-1}\\Lambda Q^T w^* $.   \n",
    "由此能看到权重衰减的效果就是沿着H的特向量所定义的轴缩放$w^*$，即根据$\\frac{\\lambda_i}{\\lambda_i + \\alpha}$因子缩放与第i个特征向量所对应的$w^*$的分量，由此可以发现**特征值较大的方向缩减较小，特征值较小的方向缩减较大**，如下图所示：    \n",
    "    <img src=\"img/l2.png\" width=320 height=320>\n",
    "实线表示目标函数的等值线，虚线表示L2正则项的等值线，对于前者，梯度比较大的方向对对该方向的移动（参数的缩减）比较敏感，特征值较大（较小的参数变化就能有较大的影响，说明对应的变量变化范围较大，即该方向特征向量对应的特征值较大），而梯度较小的方向（横向）对该方向的移动没那么敏感，特征值较小，所以为了减小L2正则项，容易将该方向参数缩减为0。    \n",
    "\n",
    "### 7.1.2 $L^1$参数正则化    \n",
    "也能如7.1.1中通过泰勒展开到用转换为元素级表示通过l1正则化最终得到的优化参数的含义，具体见书。    \n",
    "L1正则化会产生更为稀疏的解，相对应的L2并不会稀疏，所以L1正则化通常广泛的用于特征选择（Lasso，在ESL中有详细的介绍以及与Lasso相关的若干特征选择方法）。     \n",
    "此外，从最大后验估计（MAP）角度解释，L1正则化相当于权重先验为各向同性的拉普拉斯分布的对数先验，拉普拉斯的分布如下：    \n",
    "&emsp; $ f(x) = \\frac{1}{2\\lambda} e^{-\\frac{|x-\\mu|}{\\lambda}}$    \n",
    "\n",
    "## 7.2 作为约束的范数惩罚      \n",
    "通过构建广义的Lagrange函数在目标函数上加上一系列约束惩罚想，每个惩罚是一个被称为KKT乘子的系数以及一个表示约束是否满足的函数之间的乘积：    \n",
    "&emsp; $\\zeta(\\theta,\\alpha;X,y) = J(\\theta;X,y)+\\alpha (\\Omega(\\theta)-k)$     \n",
    "当$\\Omega(\\theta)>k$时，$\\alpha$必须减小，当$\\Omega(\\theta)<k$时，$\\alpha$必须增加，此外，较大的$\\alpha$对应较小的约束区域，较小的$\\alpha$对应较大的约束区域。    \n",
    "\n",
    "## 7.3 正则化和欠约束问题    \n",
    "当数据分布在一些方向上没有差异，或者数据样本过少导致在一些方向上没有方差时，该输入矩阵就是奇异的，对应没有闭式解，可能是欠定的，对于一个欠定问题，可能存在多个解，如w是完美解，那么2w,3w,...都会是完美解，如此造成数值上溢，所以，我们加入正则化，$X^TX + \\alpha I$对应求逆，即伪逆：    \n",
    "&emsp;$ X^+ = \\lim \\limits_{\\alpha \\searrow 0}(X^TX + \\alpha I)^{-1}X^T$     \n",
    "\n",
    "## 7.4 数据集增强。  \n",
    "图像中的平移、旋转、缩放，语音的加入噪声等。    \n",
    "\n",
    "## 7.5 噪声鲁棒性    \n",
    "向输入中添加极小的噪声等价于对权重施加范数惩罚，此外还有向隐藏层加入噪声（dropout）以及将噪声加入到权重（循环神经网络）。    \n",
    "例如在输入中加入随机扰动$\\epsilon_w \\sim N(\\epsilon;0,\\eta I)$后，最小化带权重噪声（方差为$\\eta I$)的J等同于最小化附加正则化项，这种形式的正则化鼓励参数进入小的权重扰动对输出影响相对较小的参数空间区域，即** 找到的点不仅是极小点，还是由平坦区域所包围的极小点**     \n",
    "** 向输出目标中注入噪声**：由于大多数数据集y标签是有一定误差的，如果明确目标（即y非0即1），那么用softmax函数进行最大似然学习时永远不会收敛，为防止这种情况，使用标签平滑，即将分类目标从0和1替换程$\\frac{\\epsilon}{k-1}$和$1-\\epsilon$, 从而防止模型追求确切概率。     \n",
    "## 7.6 多任务学习    \n",
    "通过合并几个任务中的样例（可以视为对参数施加的软约束）来提高繁华的一种方式，当模型的一部分被多个额外的任务共享时，这部分将被约束为良好的值。于是一个模型可以分为两类参数:    \n",
    "(1) 具体任务的参数    \n",
    "(2) 所有任务共享的通用参数     \n",
    "## 7.7提前终止     \n",
    "提前终止的额外代价：    \n",
    "    * 要定期评估验证集\n",
    "    * 需要保持最佳的参数副本     \n",
    "提前终止需要验证集，这意味着某些训练数据不能被模型学习，为此，我们可以在提前终止后再次训练，具体有两种策略。    \n",
    "1. 利用第一轮提前终止确定的步数i,重新在全部的数据上训练模型（参数重置），直到i步\n",
    "2. 利用第一轮训练的参数，继续在全部数据上训练，直到在验证集上的平均损失低于提前终止时的损失（会有数据泄露？）    \n",
    "### 7.7.1提前终止为何具有正则化效果\n",
    "提前终止和L2正则化很相似，如下图，左边的$\\tilde w$表示提前终止在目标函数的位置，没有接近最低点，但是离原点较近，和右图有一定约束范围的的参数空间于目标函数相交的点一样。    \n",
    "    <img src=\"img/earlier.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
