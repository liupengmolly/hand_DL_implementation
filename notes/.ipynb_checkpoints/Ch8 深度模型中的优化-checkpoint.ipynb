{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 深度模型中的优化\n",
    "## 8.1 批量算法和小批量算法\n",
    "机器学习中通常只用训练集整个代价函数中的一部分项来估计代价函数的期望值，原因如下：    \n",
    "1. 因为整个代价函数的计算代价非常大，且回想n个样本均值的标准差是$\\frac{\\delta}{\\sqrt{n}}$,可以看到使用更多样本误差减小的回报是低于线性的，如10000个样本计算量是100个样本的100倍，但是标准差却只降低10倍，所以使用小批量算法收敛更快。    \n",
    "2. 训练集冗余，大部分样本对梯度作出相似的贡献，所以抽取小样本，能保证不冗余。     \n",
    "此外，为了保证无偏估计，小批量中的样本要保持独立，且相邻梯度的样本也要保持独立，不独立意味着采样不随机，会造成偏差过大。     \n",
    "**关于无偏估计**：训练集只有在第一遍被遍历时，才是无偏估计，遵循真实泛化误差的梯度，当到第二遍时，由于它重新抽取已经用过的样本，将会是有偏的，当然，额外的遍历能减小训练误差。       \n",
    "\n",
    "## 8.2 神经网络优化中的挑战       \n",
    "\n",
    "### 8.2.1 病态      \n",
    "主要指Hessian矩阵H的病态。     \n",
    "在对代价函数用泰勒展开时：     \n",
    "&emsp; $f(x_0-\\epsilon g) = f(x_0) - f'(x_0)\\epsilon g + \\frac{1}{2} f''(x_0)(\\epsilon g)^2$        \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$= f(x_0) - g^T \\epsilon g + \\frac{1}{2} g^T H g$    \n",
    "这样在参数移动$\\epsilon g$,梯度会下降$ \\frac{1}{2} \\epsilon^2 g^T H g - \\epsilon g^T g$,当$\\frac{1}{2} \\epsilon^2 g^T H g$大于$\\epsilon g^T g$时，梯度的病态会存在问题，通常梯度范数不会随着训练过程显著减小，甚至增大（不同于正常的理解，认为训练越久，梯度越小），但是$g^T Hg$的增长会超过一个数量级，所以尽管梯度很强，代价函数下降仍很慢，这时候必须**收缩学习率。**    \n",
    "\n",
    "### 8.2.2 局部极小值       \n",
    "如果一个足够大的训练集可以唯一确定一组模型参数，那么该模型被称为**可辨认**的。带有潜变量（如神经网络）的模型通常是不可辨认的，因为通过交换潜变量能得到等价的模型（如交换神经网络单元i和j的传入权重向量和传出权重向量**？**），所以神经网络会有很多歌局部最小点，但是这些局部最小点都具有相同的代价函数值，所以不是问题。     \n",
    "当局部极小值比全局最小值大很多时，就会有隐患，即使这样，实际中的网络也不是寻找全局最小值，而是找到一个代价很小的局部最小点。     \n",
    "\n",
    "### 8.2.3 鞍点  \n",
    "多类随机函数表现出以下性质：**低维空间中，局部极小值很普遍，更高维空间中，局部极小值很罕见，而鞍点很常见**，对应于Hessian矩阵很好理解，鞍点处Hessian矩阵同时具有正负特征值，当然随着维度越高概率越大。      \n",
    "鞍点对牛顿法影响较大，因为牛顿法旨在求解梯度为0的点，这样很有可能会带向鞍点，相比之下梯度下降不会受如此大的影响，因为其旨在朝下坡移动。     \n",
    "\n",
    "### 8.2.4 悬崖和梯度爆炸            \n",
    "多层网络通常存在像悬崖一样的斜率较大区域，对此，很小的步长也可能会带来严重的问题，对此可以使用**启发式梯度截断**，其基本思想源自梯度并没有指明最佳步长，而只说明在无限小区域内的最佳方向，然后通过截断干涉来减小步长。      \n",
    "\n",
    "### 8.2.5 局部和全局结构间的弱对应       \n",
    "局部的优化方向并不是在全局上最好的优化方向。      \n",
    "\n",
    "## 8.3 基本算法     \n",
    "### 8.3.1 随机梯度下降      \n",
    "随机梯度下降的关键是学习率，保证SGD收敛的一个充分条件是：    \n",
    "&emsp; $ \\sum \\limits_{k=1}^{\\infty} \\epsilon_k = \\infty$ 且$\\sum \\limits_{k=1}^{\\infty} \\epsilon_k^2 = \\infty$    \n",
    "通常让学习率随着训练轮数降低，初始学习率的选取通常是用若干学习率训练几轮，选择比最好效果对应的学习率稍大的学习率。    \n",
    "### 8.3.2 动量       \n",
    "动量更新公式：     \n",
    "    &emsp;$ v = \\alpha v - \\epsilon g$     \n",
    "    &emsp;$ \\theta = \\theta + v$     \n",
    "形象化上理解的含义为累积固定方向上的梯度，达到在正确方向上的加速下降，如图所示：    \n",
    "    <img src=\"img/moment_gd.png\" width=300 height=300>     \n",
    "此外，假设参数更新中总是观察到梯度g, 那么会在-g上不停的加速，其步长会接近(等比公式的极限）：    \n",
    "    &emsp; $ \\frac{\\epsilon \\lVert g \\rVert}{1-\\alpha}$       \n",
    "可以理解为动量的超参数为$\\frac{1}{1-\\alpha}$,所以$\\alpha = 0.9$对应着最大速度10倍于梯度下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
