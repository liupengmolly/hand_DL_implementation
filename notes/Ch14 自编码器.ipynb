{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**自编码器，顾名思义，自己对自己进行编码，即输入等于输出，具体网络先经过一个编码层得到中间表示h,然后经过解码层，恢复，这样学习到的中间层h可以作为输入的一种近似表示，学习到有用的特征**。  \n",
    "# 14.1 欠完备自编码器\n",
    "**欠完备**，即中间层的维度小于输入，这样强迫网络学习到重要的特征，对应的要注意，当编码层和解码层太大以及中间层维度大于输入时，网络难以学习到有用的特征。    \n",
    "# 14.2 正则自编码器      \n",
    "引入正则化，这样即使是过完备的，也能通过正则化约束特征的有效性，从而得到有效的特征。\n",
    "## 14.2.1稀疏自编码器\n",
    "顾名思义，在输入输出的loss上加稀疏正则化。      \n",
    "## 14.2.2 去噪自编码器       \n",
    "即将输入加上噪声，这样避免网络学到一个恒等的编码层与解码层表示，而是强迫学习到有用特征而丢掉噪声。    \n",
    "## 14.2.3 收缩自编码器      \n",
    "同样在损失函数中加入正则化，但不同于稀疏自编码器，加入特征参数的L2正则。       \n",
    "## 14.3 表示能力、层的大小和深度      \n",
    "由万能近似定理可得，至少有一层隐藏层且隐藏单元足够多的前馈神经网络能以任意精度近似任意函数，但是其计算复杂度随之增长，且过于大（宽）的隐藏层虽然可以学习到任意逼近的函数，但是过多的参数容易导致过拟合，而逼近训练数据的函数不一定能泛化到测试数据。      \n",
    "简单而言，增加深度能较少模型复杂度，降低计算成本。所以一般训练深度自编码器。     \n",
    "## 14.4 自编码器的应用     \n",
    "自编码器主要的作用是将数据降维表示，而低维能提高许多任务的性能，小的模型消耗更少的内存和运行时间，且会将语义相关的样本至于更邻近的位置。     \n",
    "此外，信息检索也从降维中获益，将要检索的条目训练降维成一个低维且二值的编码，这种通过降维和二值化的信息检索方法称为语义哈希。         \n",
    "**trick**:信息检索中，通常在最后层使用sigmoid编码函数产生语义哈希的二值编码，所以sigmoid通常被设置为饱和，即接近0或1，窍门是在其前注入加性噪声，由于其大小随时间增加，网络为对抗这种噪声，必须加大输入到sigmoid函数的幅度，直到饱和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
