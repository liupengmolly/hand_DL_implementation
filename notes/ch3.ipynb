{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3 概率论与信息论\n",
    "## 3.1 Bernoulli分布     \n",
    "&emsp;$ P(x=1)= \\phi $  \n",
    "&emsp;$ P(x=0) = 1- \\phi$    \n",
    "&emsp;$ P(x=x) = {\\phi}^x{\\phi}^{1-x}$    \n",
    "&emsp;$ E(x) = \\phi $    \n",
    "&emsp;$ Var(x) = \\phi(1-\\phi) $    \n",
    "## 3.2 Multinouli分布，高斯分布，多维高斯分布\n",
    "## 3.3 指数分布和laplace分布\n",
    "1. 在深度学习中，经常会需要一个在x=0处取得边界点的分布，可以使用指数分布：\n",
    "&emsp;$ p(x;\\lambda) = \\lambda 1_{x\\geq 0}exp(-\\lambda x)$    \n",
    "其中$1_{x\\geq 0}$使得当x取负值时概率为0。\n",
    "2. 与指数分布联系紧密的Laplace分布，允许在任意一点$\\mu$设置概率质量的峰值：    \n",
    "&emsp; $ Laplace(x;\\mu,\\gamma) = \\frac{1}{2\\gamma}exp(-\\frac{\\mid x-\\mu \\mid}{\\gamma}) $     \n",
    "\n",
    "##3.4 Dirac分布    \n",
    "&emsp; $\\sigma(x) = \\begin{cases}+\\infty&x=0\\\\0&x=otherwise\\end{cases}$\n",
    "##3.5 混合模型    \n",
    "混合模型是组合简单概率分布来生成更丰富的分布的一种简单策略，会用到一个非常重要的概念——潜变量。例如常见的**高斯混合模型**（高斯混合模型是概率密度的万能近似器）。\n",
    "##3.6 softplus函数    \n",
    "&emsp;$\\zeta(x)=log(1+exp(x))$    \n",
    "其函数名源于它是$x^+=max(0,x)$的软化形式    \n",
    "##3.7 一些有用的性质    \n",
    "&emsp; $1-\\sigma(x) = \\sigma(-x)$    \n",
    "&emsp; $log\\sigma(x) = -\\zeta(-x)$    \n",
    "&emsp; $\\frac{d}{dx}\\zeta(x) = \\sigma(x)$  \n",
    "&emsp; $\\zeta(x)=\\int_{-\\infty}^{x} \\sigma(y)dy$    \n",
    "&emsp; $\\zeta(x)-\\zeta(-x) = x$    \n",
    "##3.8 信息论\n",
    "1. **自信息**    \n",
    "&emsp; $I(x)=-logP(x)$    \n",
    "2. **分布的香农熵**   \n",
    "&emsp; $ H(x) = E_{x\\sim P}[I(x)]$   \n",
    "一个分布的香农熵是指遵循这个分布的时间所产生的期望信息总量，确定性的分布具有较低的熵，接近均匀分布的概率具有较高的熵。    \n",
    "3. **KL散度**    \n",
    "当对同一个随机变量x有两个单独的概率分布P(x)和Q(x)，可以使用KL散度衡量两个分布的差异:    \n",
    "&emsp; $D_{KL}({P\\parallel Q}) = E_{x\\sim P}[log\\frac{P(x)}{Q(x)}]$    \n",
    "KL散度是不对称的，假设我们有一个分布p(x),希望用另一个分布q(x)来近似它，可以选择最小化$D_{KL}(p\\parallel q)$或$D_{KL}(q\\parallel p)$，前者让q在真是分布p防止高概率的地方都为高概率，后者让q在真实分布为低概率的地方都为低概率。    \n",
    "4. **交叉熵**    \n",
    "&emsp; $ H(P,Q) = -E_{x\\sim P}logQ(x)$   \n",
    "与KL散度密切联系： $H(P,Q) = H(P)+D_{KL}(P\\parallel Q)$,针对Q最小化交叉熵等价于最小化KL散度，因为Q并不包含于被省略的那一项。 \n",
    "\n",
    "##3.9 结构化概率模型    \n",
    "为降低联合分布表示的成绷，找到一种使每个因子分布具有更少变量的分解方法，而使用到图的概念（由一些可以通过边互相连接的顶点的集合构成）时，即称为结构化概率模型或者图模型。分为有向和无向的。  \n",
    "  \n",
    "* 有向： 边是有向的，所以随机变量之间能通过条件概率表示，如：     \n",
    "&emsp; $p(a,b,c,d,e) = p(a)p(b|a)p(c|a,b)p(d|b)p(e|c)$    \n",
    "* 无向： 边是无向的，通常计算无向模型中的每个团，如：    \n",
    "&emsp; $p(a,b,c,d,e) = \\frac{1}{Z} \\phi ^{(1)}(a,b,c)\\phi ^{(2)}(b,d)\\phi ^{(3)}(c,e)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
